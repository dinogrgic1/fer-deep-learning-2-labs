{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2bDZiwbUCwl"
   },
   "source": [
    "# Duboko učenje - laboratorijska vježba - generativni modeli - Boltzmannovi strojevi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCo_neEoUCwp"
   },
   "source": [
    "U ovoj vježbi pozabavit ćete se s generativnim modelima. Njihova glavna razlika u odnosu na diskriminativne modele je u tome što su predviđeni za generiranje uzoraka karakterističnih za distribuciju uzoraka korištenih pri treniranju. Da bi to mogli raditi na odgovarajući način, nužno je da mogu naučiti bitne karakteristike uzoraka iz skupa za treniranje. Jedna moguća reprezentacija tih bitnih karakteristika je distribucija ulaznih vektora, a model bi uz pomoć takve informacije mogao generirati više uzoraka koji su vjerojatniji (više zastupljeni u skupu za treniranje), a manje uzoraka koji su manje vjerojatni.\n",
    "\n",
    "Distribucija uzoraka iz skupa za treniranje može se opisati distribucijom vjerojatnosti više varijabli\n",
    "$p(\\mathbf x)$. Vjerojatnost uzoraka za treniranje $\\mathbf x^{(i)}$ trebala bi biti visoka dok bi vjerojatnost ostalih uzoraka trebala biti niža. Nasuprot tome, diskriminativni modeli se, na više ili manje izravne načine, fokusiraju na aposteriornu vjerojatnost razreda $d$\n",
    "\n",
    "$$\n",
    "p(d\\vert \\mathbf{x})=\\frac{p(d)p(\\mathbf{x}\\vert d)}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Gornji izraz sugerira da bi poznavanje $p(\\mathbf x)$ mogla biti korisna informacija i za diskriminativne modele, iako je oni u pravilu ne koriste direktno. Ipak, logična je pretpostavka da bi preciznije poznavanje $p(\\mathbf x)$ moglo pomoći u boljoj procjeni $p(d \\vert \\mathbf{x})$. Tu ideju dodatno podupire i razumna pretpostavka, da su i ulazni uzorci i odgovarajući razred $d$ (izlaz), posljedica istih bitnih značajki. Ulazni uzorci sadrže u sebi znatnu količinu bitnih informacija, ali često sadrže i šum koji dodatno otežava modeliranje direktne veze sa izlazom. Model veze izlaza i bitnih značajki, očekivano je jednostavniji nego direktna veza ulaza i izlaza.\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "      <img src=\"https://dlunizg.github.io/assets/lab4/bitneZ.svg\" width=\"30%\">\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Ovakva razmišljanja upućuju na upotrebu generativnih modela za ekstrakciju bitnih značajki. Njihova primarna namjena - generiranje uzoraka - tada je u drugom planu. Nakon treniranja, na njih se može nadograditi dodatni diskriminativni model (npr. MLP) koji na temelju bitnih značajki \"lako\" određuje željeni izlaz. Ova vježba fokusira se na treniranje generativnih modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXbOwgZoUCwq"
   },
   "source": [
    "### Ograničeni Boltzmanov stroj (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03nxy1PuUCwr"
   },
   "source": [
    "Boltzmanov stroj (BM) je [stohastička](https://en.wikipedia.org/wiki/Stochastic_neural_network) [rekurzivna](https://en.wikipedia.org/wiki/Recursive_neural_network) [generativna](https://en.wikipedia.org/wiki/Generative_model) mreža koja treniranjem nastoji maksimizirati $p(\\mathbf x^{(i)})$, a temelji se na Boltrmanovoj distribuciji prema kojoj je vjerojatnost stanja $\\mathbf x$ to manja, što je veća njegova energija $E(\\mathbf x)$ prema sljedećem izrazu\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x})\\propto\n",
    "e^{\\frac{-{E(\\mathbf{x})}}{\\mathit{kT}}}\n",
    "$$\n",
    "\n",
    "Umnožak Boltzmanove konstanta $k$ i termodinamičke temperature $T$ se ignorira, odnosno postavlja na 1.\n",
    "\n",
    "Pojedini elementi stanja BM-a $x_j$ su binarni i mogu poprimiti vrijednosti 0 i 1. Energetska funkcija $E(\\mathbf x)$ kod BM-a određena je elementima stanja $x_j$ i težinama veza $w_{ji}$ između njih te pripadajućim pomacima $b_j$.\n",
    "\n",
    "$$\n",
    "E(\\mathbf{x})=-\\left(\\frac{1}{2}\\sum _{i=1}^{N}\\sum\n",
    "_{\\substack{j=1 \\\\ j\\neq i}}^{N}w_{\\mathit{ji}}x_{j}x_{i}+\\sum\n",
    "_{j=1}^{N}b_{j}x_{j}\\right)=-\\left(\\frac{1}{2}\\mathbf{x^{T}Wx}+\\mathbf{b^{T}x}\\right)\n",
    "$$\n",
    "\n",
    "Matrica $\\mathbf{W}$ je simetrična te ima nule na glavnoj dijagonali. Vjerojatnost pojedinog uzorka definiramo kao \n",
    "\n",
    "$$\n",
    "p(\\mathbf{x};\\mathbf{W},\\mathbf{b})=\\frac{e^{-E(\\mathbf{x})/T}}{\\sum_{\\mathbf{x}}e^{-E(\\mathbf{x})/T}}=\\frac{e^{\\frac{1}{2}\\mathbf{x^{T}Wx}+\\mathbf{b^{T}x}}}{Z(\\mathbf{W},\\mathbf{b})}\n",
    "$$\n",
    "\n",
    "$Z(\\mathbf W)$ se naziva particijska funkcija, a uloga joj je normaliziranje vjerojatnosti kako bi\n",
    "\n",
    "$$\n",
    "\\sum_{\\mathbf{x}}p(\\mathbf{x};\\mathbf{W},\\mathbf{b})=1\n",
    "$$\n",
    "\n",
    "Prema odabranoj energetskoj funkciji i Boltzmanovoj distribuciji određena je vjerojatnost da pojedini element mreže ima vrijednost 1\n",
    "\n",
    "$$\n",
    "p(x_{j}=1)=\\frac{1}{1+e^{-\\sum\n",
    "_{i=1}^{N}w_{\\mathit{ji}}x_{i}-b_{j}}}=\\sigma \\left(\\sum\n",
    "_{i=1}^{N}w_{\\mathit{ji}}x_{i}+b_{j}\\right)\n",
    "$$\n",
    "\n",
    "Kako bismo energetskom funkcijom BM-a mogli opisati korelacije višeg reda, odnosno kompleksnije međusobne veze pojedinih elemenata vektora podataka, uvodimo tzv. skrivene varijable $h$. Stvarne podatke nazivamo vidljivim slojem i označavamo s $\\mathbf v$, dok skrivene varijable čine skriveni sloj $\\mathbf h$.\n",
    "\n",
    "$$\n",
    "\\mathbf{x}=(\\mathbf v,\\mathbf h)\n",
    "$$\n",
    "\n",
    "RBM je mreža u kojoj nisu dozvoljene međusobne povezanosti unutar istog sloja. To ograničenje (od tuda ime Restricted Boltzman Machine) omogućuje jednostavno osvježavanje stanja mreže. Iako ima poznatu svrhu, skriveni sloj $\\mathbf h$ i njegova distribucija $p(\\mathbf h)$ nisu poznati. \n",
    "\n",
    "<center>\n",
    "<div class=\"fig figcenter fighighlight\">\n",
    "  <img src=\"https://dlunizg.github.io/assets/lab4/rbm.svg\" width=\"10%\">\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Energija mreže sada postaje\n",
    "\n",
    "$$\n",
    "E(\\mathbf{v},\\mathbf{h})=-\\mathbf{v^{T}Wh}-\\mathbf{b^{T}h}-\\mathbf{a^{T}v}\n",
    "$$\n",
    "\n",
    "Matrica $\\mathbf W$ sadrži težine veza između vidljivog i skrivenog sloja i više nije simetrična, a vektori $\\mathbf a$ i $\\mathbf b$ sadrže pomake vidljivog i skrivenog sloja.\n",
    "Prema novoj strukturi i prethodnoj jednadžbi za vjerojatnost pojedinog elementa dobivamo dvije jednadžbe za osvježavanje stanja RBM-a. \n",
    "\n",
    "$p(v_{i}=1)=\\sigma \\left(\\sum\n",
    "_{j=1}^{N}w_{\\mathit{ji}}h_{j}+a_{i}\\right)$ za vidljivi sloj\n",
    "\n",
    "$p(h_{j}=1)=\\sigma \\left(\\sum\n",
    "_{i=1}^{N}w_{\\mathit{ji}}v_{i}+b_{j}\\right)$ za skriveni sloj\n",
    "\n",
    "Uzorkovanje vrijednosti pojedine varijable provodi se prema gornje dvije jednadžbe i pomoću generatora slučajnih brojeva.\n",
    "\n",
    "```python\n",
    "sampled_tensor = probability_tensor.bernoulli()\n",
    "```\n",
    "\n",
    "**Učenje RBM-a**\n",
    "\n",
    "Prisjetimo se da želimo maksimizirati vjerojatnost svih uzoraka za učenje (stvaranih podatka) koji su u RBM-u predstavljeni vidljivim slojem. Stoga maksimiziramo umnožak svih $p(\\mathbf{v}^{(j)})$ gdje je \n",
    "\n",
    "$$\n",
    "p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b})=\\sum\n",
    "_{\\mathbf{h}}p(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})=\\sum\n",
    "_{\\mathbf{h}}{\\frac{e^{\\mathbf{v}^{T}\\mathbf{W}\\mathbf{h}+\\mathbf{b^{T}h}+\\mathbf{a^{T}v}}}{Z(\\mathbf{W},\\mathbf{a, b})}}\n",
    "$$\n",
    "\n",
    "Maksimiziramo logaritam umnoška vjerojatnosti svih vidljivih vektora.\n",
    "\n",
    "$$\n",
    "\\ln \\left[\\prod\n",
    "_{n=1}^{N}p(\\mathbf{v}^{(n)};\\mathbf{W},\\mathbf{a},\\mathbf{b})\\right]\n",
    "$$\n",
    "\n",
    "Da bismo to postigli trebamo odrediti parcijalne derivacije s obzirom na parametre mreže\n",
    "\n",
    "$\\frac{\\partial }{\\partial w_{\\mathit{ij}}}\\ln \\left[\\prod\n",
    "_{n=1}^{N}p(\\mathbf{v}^{(n)};\\mathbf{W},\\mathbf{a},\\mathbf{b})\\right]=\\sum\n",
    "_{n=1}^{N}\\left[v_{i}^{(n)}h_{j}^{(n)}-\\sum\n",
    "_{\\mathbf{v,h}}v_{i}h_{j}p(\\mathbf{v,h};\\mathbf{W},\\mathbf{a},\\mathbf{b})\\right]=N\\left[\\langle\n",
    "v_{i}h_{j}\\rangle\n",
    "_{P(\\mathbf{h}\\vert \\mathbf{v}^{(n)};\\mathbf{W},\\mathbf{b})}-\\langle\n",
    "v_{i}h_{j}\\rangle\n",
    "_{P(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}\\right]$\n",
    "\n",
    "$\\frac{\\partial }{\\partial b_{j}}\\ln \\left[\\prod\n",
    "_{n=1}^{N}p(\\mathbf{v}^{(n)};\\mathbf{W},\\mathbf{a},\\mathbf{b})\\right]=\\sum\n",
    "_{n=1}^{N}\\left[h_{j}^{(n)}-\\sum\n",
    "_{\\mathbf{v,h}}h_{j}p(\\mathbf{v,h};\\mathbf{W},\\mathbf{a},\\mathbf{b})\\right]=N\\left[\\langle\n",
    "h_{j}\\rangle\n",
    "_{P(\\mathbf{h}\\vert \\mathbf{v}^{(n)};\\mathbf{W},\\mathbf{b})}-\\langle\n",
    "h_{j}\\rangle\n",
    "_{P(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}\\right]$\n",
    "\n",
    "$\\frac{\\partial }{\\partial a_{j}}\\ln \\left[\\prod\n",
    "_{n=1}^{N}p(\\mathbf{v}^{(n)};\\mathbf{W},\\mathbf{a},\\mathbf{b})\\right]=\\sum\n",
    "_{n=1}^{N}\\left[v_{j}^{(n)}-\\sum\n",
    "_{\\mathbf{v,h}}v_{j}p(\\mathbf{v,h};\\mathbf{W},\\mathbf{a},\\mathbf{b})\\right]=N\\left[\\langle\n",
    "v_{j}\\rangle -\\langle v_{j}\\rangle\n",
    "_{P(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}\\right]$\n",
    "\n",
    "Konačni izrazi sve tri jednadžbe sadrže po dvije komponenete u kojima $\\langle \\rangle$ zagrade označavaju usrednjene vrijednosti za $N$ ulaznih uzoraka (obično je to veličina mini grupe). \n",
    "Prvi pribrojnici u konačnim izrazima odnose se na stanja mreže kada su ulazni uzorci fiksirani u vidljivom sloju. Za određivanje odgovarajućih stanja skrivenog sloja $\\mathbf{h}$ dovoljno je svaki element $h_j$ odrediti prema izrazu za $p(h_j = 1)$.\n",
    "Drugi pribrojnici odnose se na stanja mreže bez fiksnog vidljivog sloja pa se ta stanja mogu interpretirati kao nešto što mreža zamišlja na temelju trenutne konfiguracije parametara ($\\mathbf W$, $\\mathbf a$ i $\\mathbf b$). Da bi došli do tih stanja trebamo iterativno naizmjence računati nova stanja slojeva ([Gibssovo uzorkovanje](https://en.wikipedia.org/wiki/Gibbs_sampling)) prema izrazima za $p(h_j = 1)$ i $p(v_i = 1)$. Zbog izostanka međusobnih veza elemenata istog sloja, u jednoj iteraciji se prvo paralelno uzorkuju svi skriveni elementi, a nakon toga svi elementi vidljivog sloja. Teoretski, broj iteracija treba biti velik kao bi dobili ono što mreža \"stvarno\" misli, odnosno kako bi došli do stacionarne distribucije. Tada je svejedno koje početno stanje vidljivog sloja uzmemo. Praktično rješenje ovog problema je Contrastive Divergenvce (CD) algoritam gdje je dovoljno napraviti svega $k$ iteracija (gdje je $k$ mali broj, često i samo 1), a za početna stanja vidljivog sloja uzimamo ulazne uzorke. Iako je ovo odmak od teorije, u praksi se pokazalo da dobro funkcionira. Vizualizacija CD-1 algoritma dana je na slici.\n",
    "\n",
    "<!--- no judgement for center pls-->\n",
    "<center>\n",
    "    <div class=\"fig figcenter fighighlight\">\n",
    "      <img src=\"https://dlunizg.github.io/assets/lab4/CD.svg\" width=\"30%\">\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Korekcija težina i pomaka za ulazni uzorak vi, tada se može realizirati na sljedeći način:\n",
    "\n",
    "$\\Delta w_{\\mathit{ij}}= \\eta \\left[\\langle v_{i}h_{j}\\rangle ^{0}-\\langle\n",
    "v_{i}h_{j}\\rangle ^{1}\\right]$, \n",
    "$\\Delta b_{j}=\\eta \\left[\\langle h_{j}\\rangle ^{0}-\\langle h_{j}\\rangle\n",
    "^{1}\\right]$, \n",
    "$\\Delta a_{i}=\\eta \\left[\\langle v_{i}\\rangle ^{0}-\\langle v_{i}\\rangle\n",
    "^{1}\\right]$, \n",
    "\n",
    "Faktor učenja $\\eta$ obično se postavlja na vrijednost manju od 1. Prvi pribrojnik u izrazu za $\\Delta w_{\\mathit{ij}}$ često se naziva pozitivna faza, a drugi pribrojnik, negativna faza.\n",
    "\n",
    "U zadacima će se koristiti MNIST baza. Iako pikseli u MNIST slikama mogu poprimiti realne vrijednosti iz raspona [0, 1], svaki piksel možemo promatrati kao vjerojatnost binarne varijable $p(v_i = 1)$.\n",
    "Ulazne varijable tada možemo tretirati kao stohastičke binarne varjable s [Bernoulijevom razdiobom](https://en.wikipedia.org/wiki/Bernoulli_distribution) i zadanom vjerojatnosti distribucijom $p(v_i = 1)$.\n",
    "\n",
    "### U sljedećih nekoliko blokova koda nalaze se inicijalizacijske postavke i gotove pomoćne funkcije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XCIy8NXTUCww"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.distributions as tdist\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YvJyHz9kUCwx"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 100\n",
    "VISIBLE_SIZE = 784\n",
    "HIDDEN_SIZE = 100\n",
    "GIBBS_SAMPLING_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SJEl2Q3JUCwy"
   },
   "outputs": [],
   "source": [
    "def visualize_RBM_weights(weights, grid_width, grid_height, slice_shape=(28, 28)):\n",
    "    for idx in range(0, grid_width * grid_height):\n",
    "        plt.subplot(grid_height, grid_width, idx+1)\n",
    "        plt.imshow(weights[..., idx].reshape(slice_shape))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pRrX2Le7UCwy"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor()\n",
    "                               ])), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor()\n",
    "                               ])), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTIOMmtMUCwy"
   },
   "source": [
    "### 1. zadatak - Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo4--1AQUCwz"
   },
   "source": [
    "Implementirajte RBM koji koristi CD-1 za učenje. Ulazni podaci neka su MNIST brojevi. Vidljivi sloj tada mora imati 784 elementa, a skriveni sloj neka ima 100 elemenata. Kako su vrijednosti ulaznih uzoraka (slika) realne u rasponu [0 1], oni mogu poslužiti kao $p(v_i = 1)$ pa za inicijalne vrijednosti vidljivog sloja trebate provesti uzorkovanje. Koristitie mini grupe veličine 100 uzoraka, a treniranje neka ima 100 epoha.\n",
    "\n",
    "**Podzadaci:**\n",
    "\n",
    "1. Vizualizirajte težine $\\mathbf W$ ostvarene treniranjem te pokušajte interpretirati ostvarene težine pojedinih skrivenih neurona.\n",
    "2. Vizualizirajte rezultate rekonstrukcije prvih 10 testnih uzoraka MNIST baze. Kao rezultat rekonstukcije koji ćete prikazati, koristite $p(v_{i}=1)=\\sigma \\left(\\sum_{j=1}^{N}w_{\\mathit{ji}}h_{j}+a_{i}\\right)$, umjesto binarnih vrijednosti dobivenih uzorkovanjem.\n",
    "3. Slučajno inicijalizirajte skriveni sloj, provedite nekoliko Gibbsovih uzorkovanje te vizualizirajte generirane uzorke vidljivog sloja\n",
    "4. Težine (W, v, h) spremite u datoteku \"zad1_rbm.th\" i uploadajte na Moodle.\n",
    "5. Sve tražene grafove iz podzadataka 1 i 2 predajte na Moodle.\n",
    "6. Odgovorite na sljedeće pitanje **u bilježnici**. Bilježnicu na kraju predajte na Moodle.\n",
    "\n",
    "**NAPOMENA**: Osim nadopunjavanja koda koji nedostaje, predložak se treba prilagođavati prema potrebi, a može i prema vlastitim preferencijama. Stoga **budite oprezni s tvrdnjama da vam neki dio koda ne radi!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVCJhuWvUCwz"
   },
   "source": [
    "**Pitanje:**\n",
    "\n",
    "Povećajte broj Gibsovih uzorkovanja k u CD-k. Opišite razlike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq_3lkOzUCw0"
   },
   "source": [
    "**Odgovor**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHVQMooIUCw0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SdXw7UbUUCw0"
   },
   "outputs": [],
   "source": [
    "class RBM():\n",
    "    \n",
    "    def __init__(self, visible_size, hidden_size, cd_k=1):\n",
    "        self.v_size = visible_size\n",
    "        self.h_size = hidden_size\n",
    "        self.cd_k = cd_k\n",
    "        \n",
    "        normal_dist = tdist.Normal(0, 0.1)\n",
    "        \n",
    "        self.W = torch.Tensor(normal_dist.sample(sample_shape=(self.v_size, self.h_size)))\n",
    "        self.v_bias = torch.Tensor(torch.zeros(self.v_size))\n",
    "        self.h_bias = torch.Tensor(torch.zeros(self.h_size))\n",
    "\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        return self._cd_pass(batch)\n",
    "    \n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return self.forward(batch)\n",
    "    \n",
    "    \n",
    "    def _cd_pass(self, batch):\n",
    "        batch = batch.view(-1, 784)\n",
    "        h0_prob = torch.sigmoid(batch @ self.W + self.h_bias)\n",
    "        h0 = h0_prob.bernoulli()\n",
    "\n",
    "        h1 = h0\n",
    "        \n",
    "        for step in range(0, self.cd_k):\n",
    "            v1_prob = torch.sigmoid(h1 @ self.W.T + self.v_bias)\n",
    "            v1 = v1_prob.bernoulli()\n",
    "            h1_prob = torch.sigmoid(v1 @ self.W + self.h_bias)\n",
    "            h1 = h1_prob.bernoulli()\n",
    "        \n",
    "        return h0_prob, h0, h1_prob, h1, v1_prob, v1\n",
    "    \n",
    "    def reconstruct(self, h, gibbs_steps=None):\n",
    "        h1 = h\n",
    "        \n",
    "        steps_to_do = self.cd_k\n",
    "        if gibbs_steps is not None:\n",
    "            steps_to_do = gibbs_steps\n",
    "\n",
    "        for step in range(0, steps_to_do):\n",
    "            v1_prob = torch.sigmoid(h1 @ self.W.T + self.v_bias)\n",
    "            v1 = v1_prob.bernoulli()\n",
    "            h1_prob = torch.sigmoid(v1 @ self.W + self.h_bias)\n",
    "            h1 = h1_prob.bernoulli()\n",
    "\n",
    "        return h1_prob, h1, v1_prob, v1\n",
    "\n",
    "    \n",
    "    def update_weights_for_batch(self, batch, learning_rate=0.01):\n",
    "        h0_prob, h0, h1_prob, h1, v1_prob, v1 = self._cd_pass(batch)\n",
    "        \n",
    "        w_positive_grad = h0 @ v1\n",
    "        w_negative_grad = h1 @ v1\n",
    "\n",
    "        dw = (w_positive_grad - w_negative_grad) / batch.shape[0]\n",
    "        dv_bias = (v1.T - 0) / batch.shape[0]\n",
    "        dh_bias = (h1 - h0) / batch.shape[0]\n",
    "\n",
    "        self.W = self.W + learning_rate * dw.T\n",
    "        self.v_bias = self.v_bias + learning_rate * 0\n",
    "        self.h_bias = self.h_bias + learning_rate * dh_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yYv2QVV8UCw0"
   },
   "outputs": [],
   "source": [
    "model = RBM(visible_size=VISIBLE_SIZE, hidden_size=HIDDEN_SIZE, cd_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-gvL-mhUUCw1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [17:02<00:00, 10.23s/it]\n"
     ]
    }
   ],
   "source": [
    "for curr_epoch in tqdm.tqdm(range(0, EPOCHS)):\n",
    "    for sample, label in train_loader:\n",
    "        sample = sample.view(-1, 784)\n",
    "        model.update_weights_for_batch(sample, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dcbhnyI5UCw1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36856/4272662372.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvisualize_RBM_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 12), facecolor='w')\n",
    "visualize_RBM_weights(model.W.data, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZnnzLxJUCw1"
   },
   "outputs": [],
   "source": [
    "sample, _ = next(iter(test_loader))\n",
    "sample = sample.view(-1, 784)\n",
    "\n",
    "for idx in range(0, 20): \n",
    "    h0_prob, h0, h1_prob, h1, v1_prob, v1 = model(sample)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 4), facecolor='w')\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(sample[idx, ...].view(28, 28).cpu())\n",
    "    if idx == 0:\n",
    "        plt.title(\"Original image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    recon_image = v1_prob[idx, ...].view(28, 28)\n",
    "    plt.imshow(recon_image.cpu().data)\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    state_image = h1[idx, ...].view(10, 10)\n",
    "    plt.imshow(state_image.cpu().data)\n",
    "    if idx == 0:\n",
    "        plt.title(\"Hidden state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_gtNafWUCw2"
   },
   "source": [
    "### Sljedeća dva bloka koda odnose se na inkrementalnu vizualizaciju rekonstrukcije. Ta vizualizacija je ovdje kao pomoć u intuitivnom razumijevanju funkcioniranja rekonstrukcije uzoraka i nije ju potrebno upladatai na Moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GZE_fxYUCw2"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def draw_rec(inp, title, size, Nrows, in_a_row, j):\n",
    "    plt.subplot(Nrows, in_a_row, j)\n",
    "    plt.imshow(inp.reshape(size), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "def reconstruct(ind, states, orig, weights, biases, h1_shape=(10, 10), v_shape=(28,28)):\n",
    "    j = 1\n",
    "    in_a_row = 6\n",
    "    Nimg = states.shape[1] + 3\n",
    "    Nrows = int(np.ceil(float(Nimg+2)/in_a_row))\n",
    "    \n",
    "    plt.figure(figsize=(12, 2*Nrows))\n",
    "       \n",
    "    draw_rec(states[ind], 'states', h1_shape, Nrows, in_a_row, j)\n",
    "    j += 1\n",
    "    draw_rec(orig[ind], 'input', v_shape, Nrows, in_a_row, j)\n",
    "    \n",
    "    reconstr = biases.copy()\n",
    "    j += 1\n",
    "    draw_rec(sigmoid(reconstr), 'biases', v_shape, Nrows, in_a_row, j)\n",
    "    \n",
    "    for i in range(h1_shape[0] * h1_shape[1]):\n",
    "        if states[ind,i] > 0:\n",
    "            j += 1\n",
    "            reconstr = reconstr + weights[:,i]\n",
    "            titl = '+= s' + str(i+1)\n",
    "            draw_rec(sigmoid(reconstr), titl, v_shape, Nrows, in_a_row, j)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcj43URhUCw2"
   },
   "outputs": [],
   "source": [
    "sample, _ = next(iter(test_loader))\n",
    "sample = sample[0, ...].view(-1, 784)\n",
    "\n",
    "h0_prob, h0, h1_prob, h1, v1_prob, v1 = model(sample)\n",
    "\n",
    "reconstruct(0, h1.numpy(), sample.numpy(), model.W.numpy(), model.v_bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pno4UfH7UCw2"
   },
   "outputs": [],
   "source": [
    "# Generiranje uzoraka iz slučajnih vektora\n",
    "r_input = np.random.rand(100, HIDDEN_SIZE)\n",
    "r_input[r_input > 0.9] = 1 # postotak aktivnih - slobodno varirajte\n",
    "r_input[r_input < 1] = 0\n",
    "r_input = r_input * 20 # pojačanje za slučaj ako je mali postotak aktivnih\n",
    "\n",
    "s = 10\n",
    "i = 0\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "\n",
    "h1_prob, h1, v1_prob, v1 = model.reconstruct(torch.from_numpy(r_input).float(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-SkO_K9UCw3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "for idx in range(0, 10):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(r_input[idx, ...].reshape(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Set state\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(h1[idx, ...].view(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Final state\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(v1_prob[idx, ...].view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0dYrI4PUCw3"
   },
   "outputs": [],
   "source": [
    "with open('zad1_rbm.th', 'wb') as f:\n",
    "    torch.save({\n",
    "        'rbm_w': model.W,\n",
    "        'rbm_v': model.v_bias,\n",
    "        'rbm_h': model.h_bias\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTk_KOJzUCw3"
   },
   "source": [
    "### 2. zadatak - Deep Belief Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehddMm7pUCw3"
   },
   "source": [
    "Deep belief Network (DBN) je duboka mreža koja se dobije slaganjem više RBM-ova jednog na drugi, pri čemu se svaki sljedeći RBM pohlepno trenira pomoću skrivenog (\"izlaznog\") sloja prethodnog RBM-a (osim prvog RBM-a koji se trenira direktno s ulaznim uzorcima). Teoretski, tako izgrađen DBN trebao bi povećati $p(\\mathbf v)$ što nam je i cilj. Korištenje DBN, odnosno rekonstrukcija ulaznog uzorka provodi se prema donjoj shemi. U prolazu prema gore određuju se skriveni slojevi iz vidljivog sloja dok se ne dođe do najgornjeg RBM-a, zatim se na njemu provede CD-k algoritam, nakon čega se, u prolasku prema dolje, određuju niži skriveni slojevi dok se ne dođe do rekonstruiranog vidljivog sloja. Težine između pojedinih slojeva su iste u prolazu gore kao i u prolazu prema dolje. Implementirajte troslojni DBN koji se sastoji od dva pohlepno pretrenirana RBM-a. Prvi RBM neka je isit kao i u 1. zadatku, a drugi RBM neka ima skriveni sloj od 100 elemenata.\n",
    "\n",
    "Deep Belief Network se može prikazati kao:\n",
    "<center>\n",
    "<div>\n",
    "  <img src=\"https://dlunizg.github.io/assets/lab4/DBN1.svg\" width=\"35%\">\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "\n",
    "**Podzadaci:**\n",
    "\n",
    "1. Vizualizirajte težine $\\mathbf W_2$ ostvarene treniranjem.\n",
    "2. Vizualizirajte rezultate rekonstrukcije prvih 10 testnih uzoraka MNIST baze. \n",
    "3. Spremite težine naučenog modela u datoteku \"zad2_dbn.th\" i uploadajte na Moodle.\n",
    "4. Sve grafove tražene u podzadacima 1 i 2 predajte na Moodle.\n",
    "5. Odgovorite na sljedeće pitanje **u bilježnici**. Bilježnicu na kraju predajte na Moodle.\n",
    "\n",
    "\n",
    "**NAPOMENA**: Osim nadopunjavanja koda koji nedostaje, predložak se treba prilagođavati prema potrebi, a može i prema vlastitim preferencijama. Stoga **budite oprezni s tvrdnjama da vam neki dio koda ne radi!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzfkJ6LSUCw4"
   },
   "source": [
    "**Pitanje:**\n",
    "\n",
    "Slučajno inicijalizirajte krovni skriveni sloj, provedite nekoliko Gibbsovih uzorkovanje te vizualizirajte generirane uzorke vidljivog sloja - usporedite s prethodnim zadatkom te upišite svoja zapažanja u polje \"Odgovor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV10Hf5EUCw4"
   },
   "source": [
    "**Odgovor:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSC56ofwUCw4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiHPg6soUCw4"
   },
   "outputs": [],
   "source": [
    "class DBN():\n",
    "\n",
    "    def __init__(self, first_rbm: RBM, second_hidden_size, cd_k=1):\n",
    "        self.v_size = first_rbm.v_size\n",
    "        self.h1_size = first_rbm.h_size\n",
    "        self.h2_size = second_hidden_size\n",
    "        self.cd_k = cd_k\n",
    "        \n",
    "        normal_dist = tdist.Normal(0, 0.1)\n",
    "        \n",
    "        self.W1 = first_rbm.W\n",
    "        self.v_bias = first_rbm.v_bias.clone()\n",
    "        self.h1_bias = first_rbm.h_bias.clone()\n",
    "        \n",
    "        self.W2 = torch.Tensor(normal_dist.sample(sample_shape=(self.h1_size, self.h2_size)))\n",
    "        self.h2_bias = torch.Tensor(torch.zeros(self.h2_size))\n",
    "    \n",
    "    \n",
    "    def forward(self, batch, steps=None):\n",
    "        batch = batch.view(-1, 784)\n",
    "        \n",
    "        h1up_prob =\n",
    "        h1up = \n",
    "        \n",
    "        h2up_prob = \n",
    "        h2up =\n",
    "        \n",
    "        h1down_prob, h1down, h2down_prob, h2down = self.gibbs_sampling(h2up, steps)\n",
    "        \n",
    "        return h1up_prob, h1up, h2up_prob, h2up, h1down_prob, h1down, h2down_prob, h2down\n",
    "\n",
    "    \n",
    "    def gibbs_sampling(self, h2, steps=None):\n",
    "        h2down = h2\n",
    "        \n",
    "        steps_to_do = self.cd_k\n",
    "        \n",
    "        if steps is not None:\n",
    "            steps_to_do = steps\n",
    "\n",
    "        for step in range(0, steps_to_do):\n",
    "            h1down_prob = \n",
    "            h1down = \n",
    "            \n",
    "            h2down_prob =\n",
    "            h2down =\n",
    "            \n",
    "        return h1down_prob, h1down, h2down_prob, h2down \n",
    "    \n",
    "    def reconstruct(self, h2, steps=None):\n",
    "        _, _, h2down_prob, h2down = self.gibbs_sampling(h2, steps)\n",
    "        \n",
    "        h1down_prob = \n",
    "        h1down = \n",
    "        \n",
    "        v_prob = \n",
    "        v_out = \n",
    "        \n",
    "        return v_prob, v_out, h2down_prob, h2down\n",
    "    \n",
    "    def update_weights_for_batch(self, batch, learning_rate=0.01):\n",
    "        h1up_prob, h1up, h2up_prob, h2up, h1down_prob, h1down, h2down_prob, h2down = self.forward(batch)\n",
    "\n",
    "        w2_positive_grad = \n",
    "        w2_negative_grad =\n",
    "\n",
    "        dw2 = (w2_positive_grad - w2_negative_grad) / h1up.shape[0]\n",
    "\n",
    "        self.W2 = self.W2 + \n",
    "        self.h1_bias = self.h1_bias + \n",
    "        self.h2_bias = self.h2_bias + \n",
    "                \n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return self.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArysaNssUCw4"
   },
   "outputs": [],
   "source": [
    "dbnmodel = DBN(model, second_hidden_size=100, cd_k=2)\n",
    "for curr_epoch in tqdm.tqdm(range(0, EPOCHS)):\n",
    "    for sample, label in train_loader:\n",
    "        sample = sample.view(-1, 784)\n",
    "        dbnmodel.update_weights_for_batch(sample, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn80ZwW1UCw5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12), facecolor='w')\n",
    "visualize_RBM_weights(dbnmodel.W2.data.cpu(), 10, 10, slice_shape=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18um-rb6UCw5"
   },
   "outputs": [],
   "source": [
    "sample, _ = next(iter(test_loader))\n",
    "sample = sample.view(-1, 784)\n",
    "\n",
    "for idx in range(0, 20):\n",
    "    h1up_prob, h1up, h2up_prob, h2up, h1down_prob, h1down, h2down_prob, h2down = dbnmodel(sample[idx, ...])\n",
    "    v_prob, v, _, _ = dbnmodel.reconstruct(h2down)\n",
    "\n",
    "    plt.figure(figsize=(4*3, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(sample[idx,...].view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Test input\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(v_prob[0, ...].view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(h2down.view(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Hidden state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWiVDaCcUCw5"
   },
   "outputs": [],
   "source": [
    "# Generiranje uzoraka iz slučajnih vektora\n",
    "r_input = np.random.rand(100, HIDDEN_SIZE)\n",
    "r_input[r_input > 0.9] = 1 # postotak aktivnih - slobodno varirajte\n",
    "r_input[r_input < 1] = 0\n",
    "r_input = r_input * 20 # pojačanje za slučaj ako je mali postotak aktivnih\n",
    "\n",
    "s = 10\n",
    "i = 0\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "\n",
    "v_out_prob, v_out, h2down_prob, h2down = dbnmodel.reconstruct(torch.from_numpy(r_input).float(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Sn0_7LYUCw5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "for idx in range(0, 10):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(r_input[idx, ...].reshape(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Set state\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(h2down[idx, ...].view(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Final state\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(v_out_prob[idx, ...].view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odIAiVjaUCw6"
   },
   "outputs": [],
   "source": [
    "with open('zad2_dbn.th', 'wb') as f:\n",
    "    torch.save({\n",
    "        'W1': dbnmodel.W1,\n",
    "        'v1': dbnmodel.v_bias,\n",
    "        'h1': dbnmodel.h1_bias,\n",
    "        'W2': dbnmodel.W2,\n",
    "        'h2': dbnmodel.h2_bias\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDiisgMjUCw6"
   },
   "source": [
    "### 3. zadatak - Deep belief network with generative fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvXARiekUCw6"
   },
   "source": [
    "Kako bi se dodatno poboljšala generativna svojstva DBN-a, može se provesti generativni fine-tuning parametara mreže. U 2. zadatku, prilikom rekonstruiranja korištene su iste težine i pomaci u prolascima prema dolje i prema gore. Kod fine-tuninga, parametri koji vežu sve slojeve osim dva najgornja, razdvajaju se u dva skupa. Matrice težina između nižih slojeva dijele se na: $\\mathbf R_n$ za prolaz prema gore i $\\mathbf W'_n$ za prolaz prema dolje. Inicijalno su obje matrice jednake originalnoj matrici $\\mathbf W_n$. Kod prolaza prema gore (faza budnosti - wake phase) određuju se nova stanja viših skrivenih slojeva $\\mathbf s^{(n)}$ iz nižih stanja $\\mathbf s^{(n-1)}$ pomoću matrica $\\mathbf R$ postupkom uzorkovanja ($sample \\left(\\sigma \\left(\\mathbf R_n \\mathbf s^{(n-1)} + \\mathbf b^{up}_n\\right)\\right) \\to \\mathbf s^{(n)}$) . Pri prolasku prema dolje (faza spavanja - sleep phase) određuju se \"rekonstrukcije\" nižih stanja $\\mathbf s^{(n-1)}$ iz $\\mathbf s^{(n)}$ i matrica $\\mathbf W'$ ($sample \\left( \\sigma \\left(\\mathbf W'_n \\mathbf s^{(n)} + \\mathbf b^{down}_{n-1} \\right) \\right) \\to \\mathbf s^{(n-1)}$). Najgornja dva sloja su klasični RBM i dijele istu matricu težina za prolaske u oba smjera, a modificiranje tih težina provodi se na isti način kao u 1.zadatku.\n",
    "\n",
    "Treniranje težina između nižih slojeva je drugačije. Matrice $\\mathbf W'_n$ se korigiraju kada se određuju nova stanja pomoću matrica $\\mathbf R_n$ u prolasku prema gore. U prolasku prema dolje korigiraju se matrice $\\mathbf R_n$. Vektori pomaka pojedinih slojeva $\\mathbf b_n$ se isto dijele na varijante za prolaz prema gore $\\mathbf b_n^{up}$ i za ptrolaz prem dolje $\\mathbf b_n^{down}$. Inicijalni pomaci jednaki su originalnim pomacima $\\mathbf b$.\n",
    "\n",
    "Za korekciju matrica $\\mathbf W'_n$ prilikom prolaska prema gore ($sample \\left(\\sigma \\left(\\mathbf R_n \\mathbf s^{(n-1)} + \\mathbf b^{up}_n\\right)\\right) \\to \\mathbf s^{(n)}$) provodi se i $sample \\left( \\sigma \\left(\\mathbf W'_n \\mathbf s^{(n)} + \\mathbf b^{down}_{n-1} \\right) \\right) \\to \\mathbf s^{(n-1)novo}$. Korekcija elemenata radi se na sljedeći način\n",
    "$\\Delta w'_{\\mathit{ij}}=\\eta\n",
    "s_{j}^{(n)}(s_{i}^{(n-1)}-s_{i}^{(n-1)\\mathit{novo}})$\n",
    "Korekcija pomaka za prolaz prema dolje provodi se na sljedeći način\n",
    "$\\Delta b_{\\mathit{i}}^{\\mathit{down}}=\\eta\n",
    "(s_{i}^{(n-1)}-s_{i}^{(n-1)\\mathit{novo}})$\n",
    "\n",
    "Za korekciju matrica $\\mathbf R_n$ prilikom prolaska prema dolje ($sample \\left( \\sigma \\left(\\mathbf W'_n \\mathbf s^{(n)} + \\mathbf b^{down}_{n-1} \\right) \\right) \\to \\mathbf s^{(n-1)}$) provodi se i $sample \\left(\\sigma \\left(\\mathbf R_n \\mathbf s^{(n-1)} + \\mathbf b^{up}_n\\right)\\right) \\to \\mathbf s^{(n)novo}$. Korekcija elemenata radi se na sljedeći način\n",
    "$\\Delta r_{\\mathit{ij}}=\\eta\n",
    "s_{i}^{(n-1)}(s_{j}^{(n)}-s_{j}^{(n)\\mathit{novo}})$\n",
    "Korekcija pomaka za prolaz prema dolje provodi se na sljedeći način\n",
    "$\\Delta b_{\\mathit{i}}^{\\mathit{up}}=\\eta\n",
    "(s_{i}^{(n)}-s_{i}^{(n)\\mathit{novo}})$\n",
    "\n",
    "Navedeni postupak provodi se za svaki uzorak za treniranje te se naziva up-down algoritam (ponegdje i wake-sleep algoritam).\n",
    "\n",
    "HINT: pseudokod za treniranje četveroslojnog DBN-a nalazi se u dodacima ovog [članka](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "      <img src=\"https://dlunizg.github.io/assets/lab4/DBN2.svg\" width=\"30%\">\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mrV9dcIUCw6"
   },
   "source": [
    "\n",
    "\n",
    "Implementirajte postupak generativnog fine-tuninga na DBN iz 2. zadatka. Za treniranje gornjeg RBM-a koristite CD-2.\n",
    "\n",
    "**Podzadaci:**\n",
    "\n",
    "1. Vizualizirajte konačne varijante matrica $\\mathbf W'$, $\\mathbf R$ i njihovu apsolutnu razliku.\n",
    "2. Vizualizirajte rezultate rekonstrukcije prvih 10 testnih uzoraka MNIST baze.\n",
    "3. Slučajno inicijalizirajte krovni skriveni sloj, provedite nekoliko Gibbsovih uzorkovanje te vizualizirajte generirane uzorke vidljivog sloja.\n",
    "4. Težine naučenog modela spremite i u datoteku \"zad3_dbn_ft.th\". Uploadajte tu datoteku na Moodle.\n",
    "5. Sve grafove tražene u podzadacima 1 i 2 predajte na Moodle.\n",
    "\n",
    "\n",
    "**NAPOMENA**: Osim nadopunjavanja koda koji nedostaje, predložak se treba prilagođavati prema potrebi, a može i prema vlastitim preferencijama. Stoga **budite oprezni s tvrdnjama da vam neki dio koda ne radi!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYX2geZTUCw7"
   },
   "outputs": [],
   "source": [
    "class DBNWithFineTuning():\n",
    "\n",
    "    def __init__(self, base_dbn: DBN, cd_k=1):\n",
    "        self.v_size = base_dbn.v_size\n",
    "        self.h1_size = base_dbn.h1_size\n",
    "        self.h2_size = base_dbn.h2_size\n",
    "        self.cd_k = cd_k\n",
    "        \n",
    "        normal_dist = tdist.Normal(0, 0.1)\n",
    "        \n",
    "        self.R1 = base_dbn.W1.clone()\n",
    "        self.W1_down = base_dbn.W1.T.clone()\n",
    "        self.v1_bias = base_dbn.v_bias.clone()\n",
    "        self.h1_up_bias = base_dbn.h1_bias.clone()\n",
    "        self.h1_down_bias = base_dbn.h1_bias.clone()\n",
    "        \n",
    "        self.W2 = base_dbn.W2.clone()\n",
    "        self.h2_bias = base_dbn.h2_bias.clone()\n",
    "    \n",
    "    \n",
    "    def forward(self, batch, steps=None):\n",
    "        batch = batch.view(-1, 784)\n",
    "        \n",
    "        h1_up_prob = \n",
    "        h1_up = \n",
    "        \n",
    "        v1_up_down_prob =\n",
    "        v1_up_down = \n",
    "        \n",
    "        h2_up_prob = \n",
    "        h2_up =\n",
    "        \n",
    "        h1_down_prob, h1_down, h2_down_prob, h2_down = self.gibbs_sampling(h2_up, steps=steps)\n",
    "        \n",
    "        v1_down_prob = \n",
    "        v1_down = \n",
    "        \n",
    "        h1_down_up_prob = \n",
    "        h1_down_up = \n",
    "        \n",
    "        return h1_up_prob, h1_up, v1_up_down_prob, v1_up_down, h2_up_prob, h2_up, h1_down_prob, h1_down, h2_down_prob, h2_down, v1_down_prob, v1_down, h1_down_up_prob, h1_down_up\n",
    "    \n",
    "    def gibbs_sampling(self, h2, steps=None):\n",
    "        h2_down = h2\n",
    "        \n",
    "        steps_to_do = self.cd_k\n",
    "        \n",
    "        if steps is not None:\n",
    "            steps_to_do = steps\n",
    "        \n",
    "        \n",
    "        for step in range(0, self.cd_k):\n",
    "            h1_down_prob = \n",
    "            h1_down =\n",
    "\n",
    "            h2_down_prob =\n",
    "            h2_down = \n",
    "            \n",
    "        return h1_down_prob, h1_down, h2_down_prob, h2_down\n",
    "\n",
    "\n",
    "    \n",
    "    def reconstruct(self, h2, steps=None):\n",
    "        h1_down_prob, h1_down, h2_down_prob, h2down = self.gibbs_sampling(h2, steps)\n",
    "        \n",
    "        v_out_tmp_prob =\n",
    "        v_out_tmp = \n",
    "        v_out_prob = \n",
    "        v_out = \n",
    "        \n",
    "        return v_out_prob, v_out, h2_down_prob, h2down\n",
    "    \n",
    "    def update_weights_for_batch(self, batch, learning_rate=0.01):\n",
    "        h1_up_prob, h1_up, v1_up_down_prob, v1_up_down, h2_up_prob, h2_up, h1_down_prob, h1_down, h2_down_prob, h2_down, v1_down_prob, v1_down, h1_down_up_prob, h1_down_up = self.forward(batch)\n",
    "        \n",
    "        self.W1_down = self.W1_down +\n",
    "        self.R1 = self.R1 + \n",
    "        \n",
    "        self.v1_bias = self.v1_bias +\n",
    "        \n",
    "        self.h1_down_bias = self.h1_down_bias +\n",
    "        self.h1_up_bias = self.h1_up_bias + \n",
    "        \n",
    "        \n",
    "        w2_positive_grad = \n",
    "        w2_negative_grad = \n",
    "        dw2 = (w2_positive_grad - w2_negative_grad) / h1_up.shape[0]\n",
    "        \n",
    "        self.W2 = self.W2 + \n",
    "        self.h2_bias = self.h2_bias +\n",
    "        \n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return self.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrQDtGxNUCw7"
   },
   "outputs": [],
   "source": [
    "dbnmodel_ft = DBNWithFineTuning(dbnmodel, cd_k=2)\n",
    "for curr_epoch in tqdm.tqdm(range(0, EPOCHS)):\n",
    "    for sample, label in train_loader:\n",
    "        sample = sample.view(-1, 784)\n",
    "        dbnmodel_ft.update_weights_for_batch(sample, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIOhkhIYUCw7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12), facecolor='w')\n",
    "visualize_RBM_weights(dbnmodel_ft.R1.data, 10, 10)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12), facecolor='w')\n",
    "visualize_RBM_weights(dbnmodel_ft.W1_down.T.data, 10, 10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYSpMynyUCw7"
   },
   "outputs": [],
   "source": [
    "difference = torch.abs(dbnmodel_ft.R1.data - dbnmodel_ft.W1_down.T.data)\n",
    "\n",
    "plt.figure(figsize=(12, 12), facecolor='w')\n",
    "visualize_RBM_weights(difference, 10, 10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCqTpbxYUCw8"
   },
   "outputs": [],
   "source": [
    "sample, _ = next(iter(test_loader))\n",
    "sample = sample.view(-1, 784)\n",
    "\n",
    "for idx in range(0, 20): \n",
    "    # rbn reconstruct\n",
    "    _, _, _, _, recon1, _ = model(sample[idx, ...])\n",
    "    \n",
    "    # dbn reconstruct\n",
    "    _, _, _, _, _, _, _, h2down = dbnmodel.forward(sample[idx, ...])\n",
    "    recon2, _, _, _ = dbnmodel.reconstruct(h2down)\n",
    "    \n",
    "    # dbn fine tune reconstruct\n",
    "    _, _, _, _, _, _, _, _, _, h2_down, _, _, _, _ = dbnmodel_ft(sample[idx, ...])\n",
    "    recon3, _, _, _ = dbnmodel_ft.reconstruct(h2_down, 2)\n",
    "    \n",
    "    plt.figure(figsize=(5*3, 3))\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.imshow(sample[idx, ...].view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Original image\")\n",
    "    \n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.imshow(recon1.view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction 1\")\n",
    "    \n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.imshow(recon2.view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction 2\")\n",
    "    \n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.imshow(recon3.view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction 3\")\n",
    "    \n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.imshow(h2_down.view(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Top state 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uip0BnP0UCw8"
   },
   "outputs": [],
   "source": [
    "# Generiranje uzoraka iz slučajnih vektora krovnog skrivenog sloja\n",
    "r_input = np.random.rand(100, 100)\n",
    "r_input[r_input > 0.9] = 1\n",
    "r_input[r_input < 1] = 0\n",
    "\n",
    "s = 10\n",
    "i = 0\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "i += 1\n",
    "r_input[i,:] = 0\n",
    "r_input[i,i]= s\n",
    "\n",
    "# inn = h2_downs[0:100,:]\n",
    "# out3_prob, out3, hout3 = sess3.run((v1_down_prob, v1_down, h2_down), feed_dict={h2_up: r_input})\n",
    "\n",
    "# for i in range(1000):\n",
    "#     out3_prob, out3, hout3 = sess3.run((v1_down_prob, v1_down, h2_down), feed_dict={h2_up: hout3})\n",
    "    \n",
    "v_out_prob, v_out, h2_down_prob, h2down = dbnmodel_ft.reconstruct(torch.from_numpy(r_input).float(),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o79v0I06UCw8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4 * 50))\n",
    "for idx in range(0, 50):\n",
    "    \n",
    "    plt.subplot(50, 3, 3*idx+1)\n",
    "    plt.imshow(r_input[idx, ...].reshape(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Set state\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(50, 3, 3*idx+2)\n",
    "    plt.imshow(h2down[idx, ...].view(10, 10))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Final state\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(50, 3, 3*idx+3)\n",
    "    plt.imshow(v_out_prob[idx, ...].view(28, 28))\n",
    "    if idx == 0:\n",
    "        plt.title(\"Reconstruction\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3EMw_1EUCw9"
   },
   "outputs": [],
   "source": [
    "with open('zad3_dbn_ft.th', 'wb') as f:\n",
    "    torch.save({\n",
    "        'R1': dbnmodel_ft.R1,\n",
    "        'W1_down': dbnmodel_ft.W1_down,\n",
    "        'v1': dbnmodel_ft.v1_bias,\n",
    "        'h1_up': dbnmodel_ft.h1_up_bias,\n",
    "        'h1_down': dbnmodel_ft.h1_down_bias,\n",
    "        'W2': dbnmodel_ft.W2,\n",
    "        'h2': dbnmodel_ft.h2_bias\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
