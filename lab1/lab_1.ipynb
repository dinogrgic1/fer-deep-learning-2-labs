{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1a4779",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "    $\\newcommand{\\q}{\\left}$\n",
    "    $\\newcommand{\\w}{\\right}$\n",
    "    $\\newcommand{\\m}{\\middle}$\n",
    "    $\\newcommand{\\e}{\\boldsymbol}$\n",
    "    $\\newcommand{\\cb}{\\mspace{3mu}\\m\\vert\\mspace{3mu}}$\n",
    "</div>\n",
    "\n",
    "<script type=\"text/javascript\">\n",
    "    alert(\"hello\");\n",
    "</script>\n",
    "\n",
    "<center>\n",
    "    Sveučilište u Zagrebu<br>\n",
    "    Fakultet elektrotehnike i računarstva<br>\n",
    "    <a href=\"http://www.fer.unizg.hr/predmet/dubuce\">Duboko učenje 2</a>\n",
    "</center>\n",
    "\n",
    "<h1>\n",
    "    Laboratorijska vježba 1: <br> Uvod u generativne modele\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0856874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>@font-face {\n",
       "  font-family: \"Source Serif Pro\";\n",
       "  src: url(https://fonts.googleapis.com/css?family=Souce Serif Pro);\n",
       "}\n",
       "\n",
       "div.text_cell {\n",
       "  font-family: \"Source Serif Pro\";\n",
       "  font-size: 14pt;\n",
       "}\n",
       "\n",
       "div.prompt {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.rendered_html {\n",
       "  text-align: initial;\n",
       "  line-height: 1.5em;\n",
       "  max-width: 70%;\n",
       "  width: 70%;\n",
       "  margin: auto;\n",
       "}\n",
       "\n",
       "div.text_cell p,\n",
       "div.text_cell ol,\n",
       "div.text_cell ul {\n",
       "  text-align: justify;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       "div.text_cell p {\n",
       "  text-indent: 2em;\n",
       "}\n",
       "\n",
       "div.text_cell ul + p,\n",
       "div.text_cell ol + p {\n",
       "  text-indent: 0em;\n",
       "}\n",
       "\n",
       "a.anchor-link {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.rendered_html h1,\n",
       "div.rendered_html h2 {\n",
       "  text-align: center;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "\n",
       "div.rendered_html h1:last-child,\n",
       "div.rendered_html h2:last-child {\n",
       "  margin-bottom: 0.636em;\n",
       "}\n",
       "\n",
       ".MathJax_Display {\n",
       "  margin-top: 0.5em !important;\n",
       "  margin-bottom: 0.5em !important;\n",
       "}\n",
       "div.output_area {\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "div.output_subarea {\n",
       "  max-width: 100%;\n",
       "}\n",
       "\n",
       "div.output_text {\n",
       "  width: 100%;\n",
       "  align-self: start;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# automatsko 're-importanje' modula kada se nešto izmijeni\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# podešavanje fonta i margina radi bolje čitkosti\n",
    "# odabrati File -> Trust notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "with open(\"style.css\", \"r\") as file:\n",
    "    display(HTML('<style>' + file.read() + '</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (6, 4.5)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "from graphics import plot_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f691ac3",
   "metadata": {},
   "source": [
    "## 1. Modeli mješavine\n",
    "\n",
    "<b>Modeli mješavine</b> (engl. <i>mixture models</i>) su vjerojatnosni modeli koji distribuciju podataka $\\e x$ definiraju koristeći $K$-vrijednosnu kategorijsku slučajnu varijablu $\\e z$ tako da, ovisno o realizaciji varijable $\\e z$ kao $z_k$, podatak $\\e x$ dolazi iz jedne od mogućih $K$ različitih distribucija.\n",
    "Funkcija gustoće vjerojatnosti koju opisuju je\n",
    "\\begin{equation}\n",
    "    p_{\\e \\theta}\\q(\\e x\\w) = \\sum_{k=1}^K p_{\\e \\theta}\\q(\\e x, z_k\\w) = \\sum_{k=1}^K p_{\\e \\theta}\\q(\\e x \\cb z_k\\w) \\cdot p_{\\e \\theta}\\q(z_k\\w),\n",
    "\\end{equation}\n",
    "Pripadna funkcija razdiobe varijable $\\e z$ definirana je vektorom razdiobe $\\e \\pi$:\n",
    "\\begin{equation}\n",
    "    p_{\\e \\theta}\\q(z_k\\w) = \\pi_k,\n",
    "\\end{equation}\n",
    "pri čemu vrijedi $\\pi_k \\ge 0$, za $k = 1, 2, \\ldots, K$, i $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "Uvjetna gustoća vjerojatnosti $p_{\\e \\theta}\\q(\\e x \\cb z_k\\w)$ predstavlja $k$-tu komponentu mješavine, a $\\pi_k$ ujedno daje težinu $k$-te komponente mješavine.\n",
    "Oznakom $\\e \\theta$ označava se skup svih parametara modela.\n",
    "\n",
    "Uvjetna gustoća vjerojatnosti $p_{\\e \\theta}\\q(\\e x \\cb z_k\\w)$ zadaje se tako da za različite vrijednosti $z_k$ ima isti funkcijski oblik, ali različite vrijednosti parametara.\n",
    "Izaberemo li kao $p\\q(\\e x \\cb z_k\\w)$ <a href=\"https://www.wikiwand.com/en/Normal_distribution\">univarijatnu normalnu razdiobu</a>, dobivamo model koji gustoću vjerojatnosti opisuje kao\n",
    "\\begin{equation}\n",
    "p_{\\e \\theta}\\q(x\\w) = \\sum_{k=1}^K \\pi_k \\cdot \\mathcal N \\left(x; \\mu\\q(z_k\\w), \\sigma^2\\q(z_k\\w)\\right) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2\\q(z_k\\w)}} \\cdot \\exp\\q(-\\frac{1}{2} \\cdot \\frac{\\q(x - \\mu\\q(z_k\\w)\\w)^2}{\\sigma^2\\q(z_k\\w)}\\w),\n",
    "\\end{equation}\n",
    "gdje je s $\\mathcal N \\left(x; \\mu, \\sigma^2\\right)$ označena gustoća vjerojatnosti normalne slučajne varijable s parametrima srednje vrijednosti $\\mu$ i varijance $\\sigma^2$.\n",
    "Takav model zovemo još i <b>Gaussovom mješavinom</b>.\n",
    "Kako je ovdje $\\e z$ kategorijska slučajna varijabla (koja može poprimiti konačno mnogo vrijednosti), dovoljno je definirati $\\mu\\q(\\e z\\w)$ i $\\sigma^2\\q(\\e z\\w)$ preko $K$ različitih parametara $\\mu_k$ i $\\sigma^2_k$, za $k = 1, 2, \\ldots, K$:\n",
    "\\begin{align}\n",
    "    \\mu\\q(z_k\\w) &= \\mu_k, \\\\\n",
    "    \\sigma^2\\q(z_k\\w) &= \\sigma^2_k.\n",
    "\\end{align}\n",
    "Skup svih parametara je u tom slučaju $\\e \\theta = \\q\\{ \\e \\pi, \\e \\mu, \\e {\\sigma^2} \\w\\}$, gdje su $\\e \\pi = \\q[\\pi_1, \\pi_2, \\ldots, \\pi_K \\w]$, $\\e \\mu = \\q[\\mu_1, \\mu_2, \\ldots, \\mu_K \\w]$, i $\\e {\\sigma^2} = \\q[\\sigma^2_1, \\sigma^2_2, \\ldots, \\sigma^2_K \\w]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef79e60",
   "metadata": {},
   "source": [
    "---\n",
    "<b>a)</b>\n",
    "Proučite priloženu klasu `GMDist` iz modula `utils` koja implementira distribuciju Gaussove mješavine.\n",
    "Pomoću te klase generirajte jednu distribuciju s proizvoljnim vrijednostima parametara:\n",
    " - `pi`: vektor težina komponenata $\\e \\pi$, dimenzija $K$,\n",
    " - `mu`: vektor srednjih vrijednosti $\\e \\mu$, dimenzija $K$, i\n",
    " - `sigma2`: vektor varijanci $\\e \\sigma^2$, dimenzija $K$;\n",
    "\n",
    "ili koristite metodu `GMDist.random` za generiranje neke slučajne mješavine.\n",
    "Preporučen broj komponenata mješavine je <b>između 3 i 5</b>.\n",
    "\n",
    "Generirajte slučajni uzorak iz dobivene mješavine veličine $1000000$.\n",
    "Nacrtajte na dva odvojena grafa:\n",
    " 1. histogram generiranog uzorka (funkcija `plt.hist`, parametar bins postavite dovoljno velik i uključite `density=True` da njegova površina bude 1), i\n",
    " 2. graf gustoće vjerojatnosti mješavine $p\\q(\\e x\\w)$ i njezinih komponenata $\\pi_k \\cdot p\\q(\\e x; \\mu_k, \\sigma^2_k\\w)$.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "Uvjerite se da se dobiveni grafovi podudaraju (vidite [priloženu sliku](slika_1.png) za referencu).\n",
    "Isprobajte par različitih vrijednosti parametara dok ne dobijete \"zanimljivu\" mješavinu koju ćete koristiti u podzadatcima <b>b)</b> i <b>c)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dists import GMDist\n",
    "\n",
    "K = ...\n",
    "L = 1000000\n",
    "\n",
    "dist = ...\n",
    "data = dist.sample(L)\n",
    "\n",
    "with plot_context(figsize=(12, 4.5), show=True):\n",
    "    with plot_context(subplot=(1, 2, 1), title=\"HISTOGRAM\"):\n",
    "        plt.hist  # histogram\n",
    "\n",
    "    with plot_context(subplot=(1, 2, 2), title=\"GUSTOĆA VJEROJATNOSTI\"):\n",
    "        for i in range(K):\n",
    "            plt.plot  # komponente, pomnožene pripadajućim težinama\n",
    "    \n",
    "        plt.plot  # cijela mješavina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da7030",
   "metadata": {},
   "source": [
    "U vjerojatnosnom modeliranju pretpostavlja se da skup podataka $\\mathcal D = \\q\\{ \\e x^{(1)}, \\e x^{(2)}, \\ldots, \\e x^{(N)} \\w\\}$ sadrži realizacije neke slučajne varijable $\\e x$.\n",
    "Prema kriteriju najveće izglednosti (engl. <i>maximum likelihood</i>), pri učenju modela biramo parametre $\\e \\theta$ koji daju najveću izglednost uz zadani skup podataka.\n",
    "Uvedemo li dodatnu pretpostavku da svi podatci dolaze kao realizacije iste slučajne varijable (odnosno kolekcije <a href=\"https://www.wikiwand.com/en/Iid\">međusobno nezavisnih i jednako distribuiranih slučajnih varijabli</a>), funkcija izglednosti poprima oblik\n",
    "\\begin{equation}\n",
    "    \\mathcal L\\q(\\e \\theta \\cb \\mathcal D\\w) = p_{\\e \\theta}\\q(\\e x^{(1)}, \\e x^{(2)}, \\ldots, \\e x^{(N)}\\w) = \\prod_{i=1}^N p_{\\e \\theta}\\q(\\e x^{(i)}\\w).\n",
    "\\end{equation}\n",
    "\n",
    "Funkcija izglednosti u svom izvornom obliku nezgrapna je za deriviranje, pa se stoga češće koristi njezin logaritam (oboje poprima maksimum za istu vrijednost parametara).\n",
    "K tome je u strojnom učenju uobičajeno <b>minimizirati</b> nekakvu empirijsku mjeru pogreške koja je suma gubitaka po svim primjerima, pa je zgodno definirati empirijsku pogrešku kao <b>negativan logaritam izglednosti</b>, odnosno\n",
    "\\begin{equation}\n",
    "    E\\q(\\e \\theta \\cb \\mathcal D\\w) = -\\sum_{i=1}^N \\log p_{\\e \\theta}\\q(\\e x^{(i)}\\w),\n",
    "\\end{equation}\n",
    "iz čega slijedi da je funkcija gubitka za jedan primjer $L_{\\e \\theta}\\q(\\e x^{(i)}\\w) = -\\log p_{\\e \\theta}\\q(\\e x^{(i)}\\w) $.\n",
    "\n",
    "---\n",
    "<b>b)</b>\n",
    "Dovršite implementaciju modela Gaussove mješavine `GMDist` iz modula `tf_utils` &mdash; dovršite sljedeće funkcije:\n",
    " - `loss(data)` &mdash; računa gubitak (ili vektor gubitaka) jednog primjera (ili vektora primjera),\n",
    " - `p_xz(x, k)` &mdash; računa gustoću vjerojatnosti primjera za $k$-tu komponentu, i\n",
    " - `p_x(x)` &mdash; računa gustoću vjerojatnosti primjera;\n",
    " \n",
    "ili napišite vlastitu implementaciju koristeći biblioteku za duboko učenje po želji (<b>tensorflow</b>, <b>pytorch</b>).\n",
    "Vašu implementaciju ćete trebati koristiti u narednim podzadatcima, stoga razmislite o tome da kōd izolirate u  klasu ili ga rasporedite kroz nekoliko funkcija.\n",
    "\n",
    "Parametri modela analogni su parametrima distribucije $\\e \\pi$, $\\e \\mu$ i $\\e \\sigma^2$, no ipak, prilikom pretraživanja prostora parametara gradijentnim spustom željeli bismo izbjeći ograničenja koja ti parametri moraju zadovoljavati, konkretno:\n",
    "\\begin{align}\n",
    "    \\pi_k &\\ge 0, \\quad \\text{za} \\, k = 1, 2, \\ldots, K, \\\\\n",
    "    \\sum_{k=1}^K \\pi_k &= 1, \\\\\n",
    "    \\sigma^2_k &> 0, \\quad \\text{za} \\, k = 1, 2, \\ldots, K.\n",
    "\\end{align}\n",
    "Zato se umjesto vektora težina komponenata $\\e \\pi$ uči vektor logaritama težina komponenata, $\\operatorname{\\mathbf{log}} \\e \\pi$ (varijabla `logpi` u priloženom kodu), a umjesto vektora varijanci $\\e {\\sigma^2}$ također vektor logaritama varijanci $\\operatorname{\\mathbf{log}} \\e \\sigma^2$ (varijabla `logvar` u priloženom kodu).\n",
    "Razlog je taj što logaritmi tih parametara smiju poprimiti bilo koju realnu vrijednost i pritom nisu međusobno vezani.\n",
    "Težine komponenata dobivaju se natrag primjenom funkcije $\\operatorname{\\mathbf{softmax}}$: $\\e \\pi = \\operatorname{\\mathbf{softmax}}\\q(\\operatorname{\\mathbf{log}}\\e \\pi\\w)$, gdje je\n",
    "\\begin{equation}\n",
    "    \\operatorname{softmax}_k\\q(\\e x\\w) = \\frac{\\exp x_k}{\\sum_{j=1}^K \\exp x_j},\n",
    "\\end{equation}\n",
    "a varijance primjenom funkcije $\\operatorname{\\mathbf{exp}}$, $\\e{\\sigma^2} = \\operatorname{\\mathbf{exp}}\\q(\\operatorname{\\mathbf{log}}\\e {\\sigma^2}\\w)$.\n",
    "\n",
    "Naposljetku, radi sprječavanja gubitka preciznosti, kao i radi bržeg treniranja, preporučeno je od početka raditi s negativnim logaritmima gustoće vjerojatnosti (umjesto samim gustoćama vjerojatnosti).\n",
    "U tom slučaju prirodno je definirati gubitke ostalih varijabli, $L_{\\e \\theta}\\q(x^{(i)} \\cb z_k\\w)$ i $L_{\\e \\theta}\\q(z_k\\w)$, kao\n",
    "\\begin{align}\n",
    "    L_{\\e \\theta}\\q(x^{(i)} \\cb z_k\\w) &= -\\log \\mathcal N \\q(x; \\mu_k, \\sigma^2_k\\w) = \\frac{1}{2} \\cdot \\q(\\log 2\\pi + \\log \\sigma^2 + \\frac{\\q(x - \\mu\\w)^2}{\\sigma^2}\\w), \\quad \\text{i} \\\\\n",
    "    L_{\\e \\theta}\\q(z_k\\w) &= -\\log \\operatorname{softmax}_k \\q(\\operatorname{\\mathbf{log}} \\e \\pi\\w) = \\log \\sum_{j=1}^K \\exp \\q(\\operatorname{\\mathbf{log}} \\pi\\w)_j - \\q(\\log \\pi\\w)_k\n",
    "\\end{align}\n",
    "pa gubitak primjera $x^{(i)}$ u terminima $L_{\\e \\theta}\\q(x^{(i)} \\cb z_k\\w)$ i $L_{\\e \\theta}\\q(z_k\\w)$ iznosi\n",
    "\\begin{align}\n",
    "    L_{\\e \\theta}\\q(x^{(i)}\\w) &= - \\log \\sum_{k=1}^K \\pi_k \\cdot \\mathcal N \\q(x^{(i)}; \\mu_k, \\sigma^2_k\\w) \\\\ \n",
    "    &= -\\log \\sum_{k=1}^K \\exp \\q(-\\q( L_{\\e \\theta}\\q(x^{(i)} \\cb z_k\\w) + L_{\\e \\theta}\\q(z_k\\w) \\w)\\w).\n",
    "\\end{align}\n",
    "Operacija $\\operatorname{LSE}\\q(\\e x\\w) = \\log \\sum_{k=1}^K \\exp x_k$ naziva se <a href=\"https://www.wikiwand.com/en/LogSumExp\">logaritam sume eksponenata</a>. Biblioteke za duboko učenje nude implementaciju te operacije (`tf.reduce_logsumexp`, `torch.logsumexp`) kod koje dolazi do minimalnog gubitka preciznosti.\n",
    "Pokušajte iskoristiti navedenu operaciju prilikom računanja pogreške; ako baš ne ide, izračunajte gustoću vjerojatnosti pa uzmite njezin negativan logaritam.\n",
    "\n",
    "U nastavku je dan kōd koji možete iskoristiti za treniranje modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd57cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GMModel\n",
    "\n",
    "model = GMModel(K)\n",
    "optimizer = tf.optimizers.Adam(1e-2)\n",
    "\n",
    "L = 1000\n",
    "data = dist.sample(L).reshape([-1, 1])\n",
    "batch_size = 1000\n",
    "num_epoch = 5000\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i in range(math.ceil(L / batch_size)):\n",
    "        chunk = data[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.reduce_mean(model.loss(chunk), axis=0)\n",
    "\n",
    "        grad = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(zip(grad, model.variables))\n",
    "    \n",
    "    display(HTML(f\"EPOCH {epoch} / {num_epoch}\"), clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba80eeb",
   "metadata": {},
   "source": [
    "<b>c)</b> Prikažite na istom grafu:\n",
    "1. zadanu funkciju gustoće,\n",
    "2. naučenu funkciju gustoće, i\n",
    "3. histogram izvučenog uzorka.\n",
    "\n",
    "Vidite <a href=\"slika_2.png\">priloženu sliku</a> za referencu.\n",
    "\n",
    "<p></p>\n",
    "Slaže li se naučena gustoća sa zadanom?\n",
    "Reprezentira li uzorak podataka zadanu distribuciju dovoljno dobro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "with plot_context(show=True, legend=[\"ZADANO\", \"NAUČENO\", \"UZORAK\"]):\n",
    "    plt.plot  # zadana gustoća\n",
    "    plt.plot  # naučena gustoća\n",
    "    plt.hist  # histogram uzorka za učenje\n",
    "    \n",
    "print(np.sum(model_pdf) * delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788c1fe",
   "metadata": {},
   "source": [
    "<b>d)</b> Prikažite na dva odvojena grafa:\n",
    " 1. zadanu gustoću vjerojatnosti mješavine i njenih pripadajućih komponenata, i\n",
    " 2. naučenu gustoću vjerojatnosti mješavine i njenih pripadajućih komponenata.\n",
    "\n",
    "Odgovaraju li komponente naučene mješavini komponentama iz zadane mješavine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "with plot_context(show=True, figsize=(12, 4.5)):\n",
    "    with plot_context(subplot=(1, 2, 1), title=\"ZADANO\"):\n",
    "        for k in range(K):\n",
    "            plt.plot  # komponente, pomnožene pripadajućim težinama\n",
    "            \n",
    "        plt.plot  # cijela mješavina\n",
    "            \n",
    "    with plot_context(subplot=(1, 2, 2), title=\"NAUČENO\"):\n",
    "        for k in range(model.K):\n",
    "            plt.plot  # komponente, pomnožene pripadajućim težinama\n",
    "            \n",
    "        plt.plot  # cijela mješavina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c709600",
   "metadata": {},
   "source": [
    "<b>e)</b>\n",
    "Naučeni model mješavine može se koristiti i za <b>grupiranje podataka</b> (<i>clustering</i>) u $K$ grupa.\n",
    "Uvjetna vjerojatnost $p_{\\e \\theta}\\q(z_k \\cb \\e x\\w)$ za $k = 1, 2, \\ldots, K$, predstavlja vjerojatnost da primjer $\\e x$ dolazi iz $k$-te komponente mješavine i računa se kao\n",
    "\\begin{equation}\n",
    "    p_{\\e \\theta}\\q(z_k \\cb \\e x\\w) = \\frac{p_{\\e \\theta}\\q(\\e x, z_k\\w)}{p_{\\e \\theta}\\q(\\e x\\w)} = \\frac{\\pi_k \\cdot p_{\\e \\theta}\\q(\\e x \\cb z_k\\w)}{\\sum_{i=1}^K \\pi_i \\cdot p_{\\e \\theta}\\q(\\e x \\cb z_i\\w)}.\n",
    "\\end{equation}\n",
    "Kriterij maksimalne izglednosti daje podjelu skupa podataka $\\mathbb X$ na $K$ disjunktnih grupa na sljedeći način:\n",
    "\\begin{equation}\n",
    "    \\mathcal G_k = \\q\\{ \\e x \\in \\mathbb X \\cb \\operatorname{arg\\,max}_i p_{\\e \\theta}\\q(z_i \\cb \\e x\\w) = k \\w\\}\n",
    "\\end{equation}\n",
    "\n",
    "Izvucite uzorak od 1000000 primjera iz prethodno zadane distribucije.\n",
    "Nacrtajte na $K$ odvojenih grafova histograme pojedinih grupa.\n",
    "Vidite <a href=\"slika_3.png\">priloženu sliku</a> za referencu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1000000\n",
    "data = dist.sample(L)\n",
    "\n",
    "_, bins = np.histogram(data, bins=1001)  # ako želimo imati iste 'koševe' u svim grafovima: plt.hist(..., bins=bins)\n",
    "\n",
    "with plot_context(show=True, figsize=(6, 4.5 * model.K / 2), suptitle=\"GRUPE\"):\n",
    "    for k in range(K):\n",
    "        with plot_context(subplot=(model.K, 1, k + 1), legend=[f\"k={k}\"]):\n",
    "            plt.hist  # histogram grupe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9e9f3",
   "metadata": {},
   "source": [
    "<b>f)</b>\n",
    "Implementirajte mješavinu (kontinuiranih) <a href=\"https://www.wikiwand.com/en/Continuous_uniform_distribution\">uniformnih distribucija</a> po uzoru na priloženu implementaciju Gaussove mješavine.\n",
    "Možete dopuniti zadanu klasu `UMDist` ili napisati vlastiti kod po želji.\n",
    "\n",
    "Ponovno nacrtajte histogram i graf gustoće vjerojatnosti kao u podzadatku <b>a)</b>.\n",
    "Koristite <b>2 do 3</b> komponente.\n",
    "Generirajte neku \"zanimljivu\" mješavinu koju ćete koristiti u sljedećem podzadatku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4154700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dists import UMDist\n",
    "\n",
    "K = ...\n",
    "L = 1000000\n",
    "\n",
    "dist = ...\n",
    "data = dist.sample(L)\n",
    "\n",
    "with plot_context(figsize=(12, 4.5), show=True):\n",
    "    with plot_context(subplot=(1, 2, 1), title=\"HISTOGRAM\"):\n",
    "        plt.hist  # histogram\n",
    "\n",
    "    with plot_context(subplot=(1, 2, 2), title=\"GUSTOĆA VJEROJATNOSTI\"):\n",
    "        for i in range(K):\n",
    "            plt.plot  # komponente, pomnožene pripadajućim težinama\n",
    "    \n",
    "        plt.plot  # cijela mješavina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82dade2",
   "metadata": {},
   "source": [
    "<b>g)</b>\n",
    "Zatim iskoristite model Gaussove mješavine da biste naučili prethodno generiranu mješavinu uniformnih distribucija.\n",
    "Varirajte broj komponenata mješavine <b>modela</b> (ne distribucije) <b>između 3 i 10</b>, te veličinu uzorka za učenje i broj epoha.\n",
    "U nastavku je dan kod za treniranje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae64f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GMModel\n",
    "\n",
    "model = GMModel(10)\n",
    "optimizer = tf.optimizers.Adam(1e-2)\n",
    "\n",
    "L = 10000\n",
    "data = dist.sample(L).reshape([-1, 1])\n",
    "batch_size = 1000\n",
    "num_epoch = 5000\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i in range(math.ceil(L / batch_size)):\n",
    "        chunk = data[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.reduce_mean(model.loss(chunk), axis=0)\n",
    "\n",
    "        grad = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(zip(grad, model.variables))\n",
    "    \n",
    "    display(HTML(f\"EPOCH {epoch} / {num_epoch}\"), clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be020f",
   "metadata": {},
   "source": [
    "<b>i)</b> Ponovite vizualizacije iz <b>c)</b> i <b>d)</b> podzadataka.\n",
    "Slaže li se naučena gustoća sa zadanom?\n",
    "Reprezentira li uzorak podataka zadanu distribuciju dovoljno dobro?\n",
    "Može li model Gaussove mješavine dobro aproksimirati i druge složene distribucije?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "with plot_context(show=True, legend=[\"ZADANO\", \"NAUČENO\", \"UZORAK\"]):\n",
    "    plt.plot  # zadana gustoća\n",
    "    plt.plot  # naučena gustoća\n",
    "    plt.hist  # histogram uzorka za učenje\n",
    "    \n",
    "print(np.sum(model_pdf) * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "with plot_context(show=True, figsize=(12, 4.5)):\n",
    "    with plot_context(subplot=(1, 2, 1), title=\"ZADANO\"):\n",
    "        for k in range(K):\n",
    "            plt.plot  # komponente, pomnožene pripadajućim težinama\n",
    "            \n",
    "        plt.plot  # cijela mješavina\n",
    "            \n",
    "    with plot_context(subplot=(1, 2, 2), title=\"NAUČENO\"):\n",
    "        for k in range(model.K):\n",
    "            plt.plot  # komponente, pomnožene pripadajućim težinama\n",
    "            \n",
    "        plt.plot  # cijela mješavina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca584c",
   "metadata": {},
   "source": [
    "## 2. Probabilistička regresija\n",
    "\n",
    "U klasičnoj regresiji, raspolažemo skupom podataka i njihovih pripadajućih oznaka $\\mathcal D = \\q\\{ \\q(\\e x^{(i)}, y^{(i)}\\w) \\w\\}$.\n",
    "Cilj je naučiti funkcijsku ovisnost između podatka $\\e x$ i njegove oznake $y$.\n",
    "Tu funkciju opisujemo neuronskom mrežom $f_{\\e \\theta}$,\n",
    "\\begin{equation}\n",
    "    y^{(i)} = f_{\\e \\theta}\\q(\\e x^{(i)}\\w) + \\epsilon^{(i)}\n",
    "\\end{equation}\n",
    "gdje $\\e \\theta$ označava skup svih parametara te mreže, a $\\epsilon^{(i)}$ slučajni šum.\n",
    "Šum može biti <a href=\"https://www.wikiwand.com/en/Measurement_error\">mjerni šum</a>, ali on također može dolaziti i od neosmotrenih (latentnih) varijabli koje utječu na oznaku $y^{(i)}$, ali njihove vrijednosti nam nisu poznate, pa je stoga model nepotpun.\n",
    "Sam šum nije dio modela, već se najbolja točkasta procjena oznake $i$-tog podatka $\\hat{y}^{(i)}$ dobiva kao\n",
    "\\begin{equation}\n",
    "    \\hat{y}^{(i)} = f_{\\e \\theta}\\q(\\e x^{(i)}\\w).\n",
    "\\end{equation}\n",
    "Definiramo proizvoljnu funkiju gubitka $L_{\\e \\theta}\\q(\\hat{y}, y\\w)$, najčešće kvadratni $\\q(y - \\hat{y}\\w)^2$ ili apsolutni gubitak $\\q\\lvert y - \\hat{y} \\w\\rvert$.\n",
    "Sada se optimalna funkcija $f_{\\e \\theta}$ može pronaći minimiziranjem empirijskog gubitka\n",
    "\\begin{equation}\n",
    "    E\\q(\\e \\theta \\cb \\mathcal D\\w) = \\sum_{i = 1}^N L_{\\e \\theta}\\q(\\hat{y}^{(i)}, y^{(i)}\\w).\n",
    "\\end{equation}\n",
    "\n",
    "U probabilističkoj regresiji, oznaku $i$-tog podatka $y^{(i)}$ tretiramo kao realizaciju slučajne varijable $\\q. y \\cb \\e x^{(i)} \\w.$.\n",
    "Zadajemo odgovarajuću parametriziranu distribuciju $p_{\\e \\theta}\\q(y \\cb \\e x\\w)$ kojom ćemo opisati te realizacije.\n",
    "U većini slučajeva to će biti normalna distribucija $\\mathcal N\\q(\\mu\\q(\\e x\\w), \\sigma^2\\q(\\e x\\w)\\w)$, ali možemo zadati i neku drugu.\n",
    "Nadalje, želimo odabrati parametre $\\e \\theta$ za koje je izglednost da generiraju dostupne oznake najveća.\n",
    "Uz iste pretpostavke kao u prethodnom zadatku, definiramo izglednost\n",
    "\\begin{equation}\n",
    "    \\mathcal L\\q(\\e \\theta \\cb \\mathcal D\\w) = \\prod_{i = 1}^N p_{\\e \\theta}\\q(y^{(i)} \\cb \\e x^{(i)}\\w),\n",
    "\\end{equation}\n",
    "odnosno empirijsku pogrešku\n",
    "\\begin{equation}\n",
    "    E\\q(\\e \\theta \\cb \\mathcal D\\w) = -\\sum_{i = 1}^N \\log p_{\\e \\theta}\\q(y^{(i)} \\cb \\e x^{(i)}\\w).\n",
    "\\end{equation}\n",
    "\n",
    "U ostatku ovog zadatka modelirat ćemo podatke u skladu sa sljedećim distribucijama:\n",
    "\\begin{align}\n",
    "    x &\\sim \\mathcal N\\q(0, 1\\w),  \\\\\n",
    "    \\left. y \\, \\middle \\vert \\, x \\right. &\\sim \\mathcal N\\left(\\mu \\left(x\\right), \\sigma^2\\left(x\\right) \\right).\n",
    "\\end{align}\n",
    "Funkcije $\\mu\\q(x\\w)$ i $\\sigma^2\\q(x\\w)$ opisat ćemo neuronskom mrežom.\n",
    "Parametri $\\e \\theta$ parametri su te mreže.\n",
    "Izlaz modela više nije točkasta procjena $\\hat{y}$, već slučajna varijabla koja nam može nešto reći i o nesigurnosti procjene, odnosno šumu kojeg klasična regresija zanemaruje.\n",
    "\n",
    "---\n",
    "<b>a)</b>\n",
    "Proizvoljno definirajte funkcije `mean_y(x)` i `sigma2_y(x)` koje opisuju ovisnost parametara $\\mu$ i $\\sigma^2$ uvjetne slučajne varijable $\\q.y \\cb x\\w.$ u ovisnosti o realizaciji slučajne varijable $x$.\n",
    "Napravite funkciju `gen_data(L)` koja generira $L$ uzoraka slučajne varijable $x$ i njima pripadnih oznaka $y$.\n",
    "\n",
    "Zatim generirajte uzorak veličine $L = 1000$ i odvojeno prikažite:\n",
    " 1. graf intervala pouzdanosti širine 1-$\\sigma$ uvjetne slučajne varijable $\\q. y \\cb x \\w.$ (to je raspon vrijednosti između $\\mu - \\sigma$ i $\\mu + \\sigma$),\n",
    " 2. graf raspršenja generiranog uzorka.\n",
    "\n",
    "Vidite <a href=\"slika_4.png\">priloženu sliku</a> za referencu.\n",
    "Varirajte funkcije `mean_y` i `sigma2_y` tako da dobijete neku \"zanimljivu\" distribuciju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = lambda x: ...\n",
    "sigma2_y = lambda x: ...\n",
    "\n",
    "L = 1000\n",
    "\n",
    "def gen_data(L):\n",
    "    X = ...\n",
    "    Y = ...\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X, Y = gen_data(L)\n",
    "\n",
    "with plot_context(show=True, figsize=(12, 4.5)):\n",
    "    with plot_context(subplot=(1, 2, 1), title=\"1-$\\sigma$ INTERVAL POUZDANOSTI\", legend=[\"$\\mu$\", \"$\\mu \\pm \\sigma$\"]):\n",
    "        plt.plot  # srednja vrijednost\n",
    "        plt.fill_between  # interval povjerenja\n",
    "\n",
    "    with plot_context(subplot=(1, 2, 2), title=\"UZORAK\"):\n",
    "        plt.scatter  # uzorak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dce6d9",
   "metadata": {},
   "source": [
    "<b>b)</b>\n",
    "Koristite neuronsku mrežu za učenje parametara uvjetne slučajne varijable $\\q. y \\cb x \\w.$.\n",
    "Varirajte broj slojeva mreže <b>između 2 i 5</b> (ne brojeći ulazni sloj), te isprobajte različite kombinacije broja čvorova u skrivenim slojevima.\n",
    "Po želji možete isprobati i različite aktivacijske funkcije u skrivenim slojevima.\n",
    "Izlazni sloj mora imati $2$ čvora, te na njega ne smije biti primijenjena aktivacija.\n",
    "Dovršite kod za treniranje, pa istrenirajte model nad generiranim uzorkom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e136bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(2, activation=None)])\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-3)\n",
    "\n",
    "for _ in range(1000):\n",
    "    X, Y = gen_data(100)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        mean, logvar = tf.split(model(X.reshape([-1, 1])), num_or_size_splits=[1, 1], axis=1)\n",
    "        loss = ...\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f499800",
   "metadata": {},
   "source": [
    "<b>c)</b> Na dva odvojena grafa prikažite interval pouzdanosti širine 1-$\\sigma$ uvjetne slučajne varijable $\\q. y \\cb x \\w.$\n",
    " 1. zadane distribucije, i\n",
    " 2. naučenog modela.\n",
    "\n",
    "Podudaraju li se grafovi?\n",
    "Može li naučeni model generirati nove primjere slične onima iz skupa za učenje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "with plot_context(show=True, figsize=(12, 4.5)):\n",
    "    with plot_context(subplot=(1, 2, 1), title=\"ZADANO\"):\n",
    "        plt.plot  # srednja vrijednost\n",
    "        plt.fill_between  # interval pouzdanosti\n",
    "        \n",
    "    with plot_context(subplot=(1, 2, 2), title=\"NAUČENO\"):\n",
    "        plt.plot  # srednja vrijednost\n",
    "        plt.fill_between  # interval pouzdanosti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ebd47",
   "metadata": {},
   "source": [
    "<b>d)</b>\n",
    "Za usporedbu istrenirajte i klasičan model regresije uz kvadratni ili apsolutni gubitak, pa na dva odvojena grafa prikažite:\n",
    " 1. interval pouzdanosti širine 1-$\\sigma$ uvjetne slučajne varijable $\\q. y \\cb x \\w.$ zadane distribucije, i\n",
    " 2. točkastu procjenu oznake $\\hat{y}$ koju daje model i graf raspršenja uzorka veličine $L = 100$.\n",
    "\n",
    "Pogađa li procjena modela približno točke u grafu raspršenja?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(1, activation=None)])\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-3)\n",
    "\n",
    "for _ in range(1000):\n",
    "    X, Y = gen_data(100)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(X.reshape([-1, 1]))\n",
    "        loss = ...\n",
    "        \n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa74b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "X, Y = gen_data(100)\n",
    "\n",
    "with plot_context(show=True, figsize=(12, 4.5)):\n",
    "    with plot_context(subplot=(1, 2, 1), title=\"ZADANO\"):\n",
    "        plt.plot  # srednja vrijednost\n",
    "        plt.fill_between  # interval pouzdanosti\n",
    "        \n",
    "    with plot_context(subplot=(1, 2, 2), title=\"NAUČENO\"):\n",
    "        plt.plot  # srednja vrijednost\n",
    "        plt.scatter  # uzorak"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
